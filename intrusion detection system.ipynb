{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb931c-9c2e-4147-81d9-a10f71528c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without yolo\n",
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tim\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "from twilio.rest import Client\n",
    "\n",
    "# Ensure the 'outputs' directory exists\n",
    "if not os.path.exists(\"outputs\"):\n",
    "    os.makedirs(\"outputs\")\n",
    "\n",
    "# Define the kernel for morphological operations\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "def is_person_present(frame, thresh=1100):\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Check if there was a contour and the area is above a threshold\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the detected person\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        return True, frame\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "    \"\"\"Sends an SMS alert using Twilio API\"\"\"\n",
    "    account_sid = info_dict['account_sid']\n",
    "    auth_token = info_dict['auth_token']\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(\n",
    "        to=info_dict['your_num'],\n",
    "        from_=info_dict['trial_num'],\n",
    "        body=body\n",
    "    )\n",
    "\n",
    "# Initialize OpenCV Window\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Load Video File\n",
    "cap = cv2.VideoCapture('sample_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7dcb6f-87e0-4106-9a37-f86e0ae953d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video recording happning+ ip address also happning\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "from twilio.rest import Client\n",
    "\n",
    "# Ensure 'outputs' directory exists\n",
    "if not os.path.exists(\"outputs\"):\n",
    "    os.makedirs(\"outputs\")\n",
    "\n",
    "# Check required files\n",
    "for file in [\"yolov4.weights\", \"yolov4.cfg\", \"coco.names\"]:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"Error: {file} not found!\")\n",
    "        exit()\n",
    "\n",
    "# Load YOLO model\n",
    "weights_path = \"yolov4.weights\"\n",
    "config_path = \"yolov4.cfg\"\n",
    "labels_path = \"coco.names\"\n",
    "\n",
    "with open(labels_path, \"r\") as f:\n",
    "    labels = f.read().strip().split(\"\\n\")\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "layer_names = net.getLayerNames()\n",
    "out_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "# Detect persons using YOLO\n",
    "def detect_person_yolo(frame, conf_threshold=0.5, nms_threshold=0.4):\n",
    "    height, width = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(out_layers)\n",
    "\n",
    "    boxes, confidences = [], []\n",
    "    person_detected = False\n",
    "    \n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if labels[class_id] == \"person\" and confidence > conf_threshold:\n",
    "                box = detection[0:4] * np.array([width, height, width, height])\n",
    "                (centerX, centerY, w, h) = box.astype(\"int\")\n",
    "                x = int(centerX - w / 2)\n",
    "                y = int(centerY - h / 2)\n",
    "                boxes.append([x, y, int(w), int(h)])\n",
    "                confidences.append(float(confidence))\n",
    "                person_detected = True\n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Person {confidences[i]:.2f}', (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    print(f\"Person detected: {person_detected}\")\n",
    "    return person_detected, frame\n",
    "\n",
    "# Twilio SMS function\n",
    "def send_message(body, info_dict):\n",
    "    account_sid = info_dict['account_sid']\n",
    "    auth_token = info_dict['auth_token']\n",
    "    client = Client(account_sid, auth_token)\n",
    "    message = client.messages.create(\n",
    "        to=info_dict['your_num'], from_=info_dict['trial_num'], body=body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d81504-df22-4bc7-8ab8-774d2c409d97",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:6\u001b[1;36m\u001b[0m\n\u001b[1;33m    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\u001b[0m\n\u001b[1;37m                                                                                 ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# sample_video  code for video saving  \n",
    "                boxes.append([x, y, int(w), int(h)])\n",
    "                confidences.append(float(confidence))\n",
    "                person_detected = True\n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Person {confidences[i]:.2f}', (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    print(f\"Person detected: {person_detected}\")\n",
    "    return person_detected, frame\n",
    "\n",
    "# Twilio SMS function\n",
    "def send_message(body, info_dict):\n",
    "    account_sid = info_dict['account_sid']\n",
    "    auth_token = info_dict['auth_token']\n",
    "    client = Client(account_sid, auth_token)\n",
    "    message = client.messages.create(\n",
    "        to=info_dict['your_num'], from_=info_dict['trial_num'], body=body\n",
    "    )\n",
    "\n",
    "# Load Sample Video\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "video_path = \"sample_video.mp4\"  # Change to actual path of the sample video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open sample video file!\")\n",
    "    exit()\n",
    "\n",
    "width, height = int(cap.get(3)), int(cap.get(4))\n",
    "\n",
    "# Detection Variables\n",
    "status = False\n",
    "patience = 7\n",
    "detection_thresh = 15\n",
    "de = deque([False] * detection_thresh, maxlen=detection_thresh)\n",
    "initial_time, start_time = None, time.time()\n",
    "fps, frame_counter = 0, 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video reached.\")\n",
    "        break\n",
    "\n",
    "    detected, annotated_image = detect_person_yolo(frame)\n",
    "    de.appendleft(detected)\n",
    "    print(f\"Detection count in deque: {sum(de)}\")\n",
    "\n",
    "    if sum(de) == detection_thresh and not status:\n",
    "        status = True\n",
    "        entry_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "        output_video_path = f'outputs/{entry_time}.mp4'\n",
    "        out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'XVID'), 15.0, (width, height))\n",
    "        print(f\"Status set to True at {entry_time}\")\n",
    "        print(f\"VideoWriter initialized at {output_video_path}\")\n",
    "\n",
    "    if status and not detected:\n",
    "        if sum(de) > (detection_thresh / 2):\n",
    "            if initial_time is None:\n",
    "                initial_time = time.time()\n",
    "        elif initial_time is not None and time.time() - initial_time >= patience:\n",
    "            status = False\n",
    "            exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "            out.release()\n",
    "            print(f\"Recording stopped at {exit_time}\")\n",
    "            initial_time = None\n",
    "    \n",
    "    if status:\n",
    "        print(\"Writing frame to video...\")\n",
    "        out.write(annotated_image)\n",
    "\n",
    "    cv2.imshow('frame', annotated_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "if status:\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f1262-5270-45a7-8d09-62b7a1434d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started: outputs/Monday, 11:13:50 AM 07 April 2025.mp4\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Recording stopped: outputs/Monday, 11:13:50 AM 07 April 2025.mp4\n",
      "Recording started: outputs/Monday, 11:16:08 AM 07 April 2025.mp4\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Recording stopped: outputs/Monday, 11:16:08 AM 07 April 2025.mp4\n",
      "Recording started: outputs/Monday, 11:17:04 AM 07 April 2025.mp4\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Writing frame to video...\n",
      "Recording stopped: outputs/Monday, 11:17:04 AM 07 April 2025.mp4\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n",
      "Error: Frame not received! Retrying...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from collections import deque\n",
    "from twilio.rest import Client\n",
    "\n",
    "# Ensure 'outputs' directory exists\n",
    "if not os.path.exists(\"outputs\"):\n",
    "    os.makedirs(\"outputs\")\n",
    "\n",
    "# Check required files\n",
    "for file in [\"yolov4.weights\", \"yolov4.cfg\", \"coco.names\"]:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"Error: {file} not found!\")\n",
    "        exit()\n",
    "\n",
    "# Load YOLO model\n",
    "weights_path = \"yolov4.weights\"\n",
    "config_path = \"yolov4.cfg\"\n",
    "labels_path = \"coco.names\"\n",
    "\n",
    "with open(labels_path, \"r\") as f:\n",
    "    labels = f.read().strip().split(\"\\n\")\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "layer_names = net.getLayerNames()\n",
    "out_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "# Detect persons using YOLO\n",
    "def detect_person_yolo(frame, conf_threshold=0.5, nms_threshold=0.4):\n",
    "    height, width = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(out_layers)\n",
    "\n",
    "    boxes, confidences = [], []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if labels[class_id] == \"person\" and confidence > conf_threshold:\n",
    "                box = detection[0:4] * np.array([width, height, width, height])\n",
    "                (centerX, centerY, w, h) = box.astype(\"int\")\n",
    "                x = int(centerX - w / 2)\n",
    "                y = int(centerY - h / 2)\n",
    "                boxes.append([x, y, int(w), int(h)])\n",
    "                confidences.append(float(confidence))\n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    person_detected = False\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Person {confidences[i]:.2f}', (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "            person_detected = True\n",
    "\n",
    "    return person_detected, frame\n",
    "\n",
    "# Twilio SMS function\n",
    "def send_message(body, info_dict):\n",
    "    account_sid = info_dict['account_sid']\n",
    "    auth_token = info_dict['auth_token']\n",
    "    client = Client(account_sid, auth_token)\n",
    "    message = client.messages.create(\n",
    "        to=info_dict['your_num'], from_=info_dict['trial_num'], body=body\n",
    "    )\n",
    "\n",
    "# Load Video\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "#  IP Camera URL\n",
    "ip_camera_url =\"http://192.168.130.118:8080/video\"\n",
    "cap = cv2.VideoCapture(ip_camera_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open IP camera! Retrying...\")\n",
    "    time.sleep(2)\n",
    "    cap = cv2.VideoCapture(ip_camera_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera connection failed! Check your network & IP camera settings.\")\n",
    "    exit()\n",
    "\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "width, height = int(cap.get(3)), int(cap.get(4))\n",
    "\n",
    "# Read credentials\n",
    "if os.path.exists('credentialss.txt'):\n",
    "    with open('credentialss.txt', 'r') as myfile:\n",
    "        data = myfile.read()\n",
    "    info_dict = eval(data)\n",
    "else:\n",
    "    print(\"Error: credentialss.txt not found!\")\n",
    "    exit()\n",
    "\n",
    "# Detection Variables\n",
    "status = False\n",
    "patience = 7\n",
    "detection_thresh = 15\n",
    "de = deque([False] * detection_thresh, maxlen=detection_thresh)\n",
    "initial_time, start_time = None, time.time()\n",
    "fps, frame_counter = 0, 0\n",
    "out = None  \n",
    "\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Frame not received! Retrying...\")\n",
    "        cap.release()\n",
    "        cap = cv2.VideoCapture(ip_camera_url)\n",
    "        continue\n",
    "\n",
    "    detected, annotated_image = detect_person_yolo(frame)\n",
    "    de.appendleft(detected)\n",
    "\n",
    "    if sum(de) == detection_thresh and not status:\n",
    "        status = True\n",
    "        entry_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "        video_path = f'outputs/{entry_time}.mp4'\n",
    "        frame_size = (frame.shape[1], frame.shape[0])\n",
    "        out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'XVID'), 15.0, frame_size)\n",
    "        print(f\"Recording started: {video_path}\")\n",
    "\n",
    "    if status:\n",
    "        print(\"Writing frame to video...\")\n",
    "        out.write(annotated_image)\n",
    "\n",
    "    if status and not detected:\n",
    "        if sum(de) > (detection_thresh / 2):\n",
    "            if initial_time is None:\n",
    "                initial_time = time.time()\n",
    "        elif initial_time is not None and time.time() - initial_time >= patience:\n",
    "            status = False\n",
    "            exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "            print(f\"Recording stopped: {video_path}\")\n",
    "            out.release()\n",
    "            initial_time = None\n",
    "            body = f\"Alert: A Person Entered at {entry_time} and Left at {exit_time}\"\n",
    "            send_message(body, info_dict)\n",
    "\n",
    "    fps = frame_counter / (time.time() - start_time + 1e-5)\n",
    "    frame_counter += 1\n",
    "    cv2.putText(annotated_image, f'FPS: {fps:.2f}', (510, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)\n",
    "    cv2.imshow('frame', annotated_image)\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "if status:\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc060e7-0f89-4f46-923e-3c35c76cb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from twilio.rest import Client\n",
    "\n",
    "# Ensure 'outputs' directory exists\n",
    "if not os.path.exists(\"outputs\"):\n",
    "    os.makedirs(\"outputs\")\n",
    "\n",
    "# Check required files\n",
    "for file in [\"yolov4.weights\", \"yolov4.cfg\", \"coco.names\"]:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"Error: {file} not found!\")\n",
    "        exit()\n",
    "\n",
    "# Load YOLO model\n",
    "weights_path = \"yolov4.weights\"\n",
    "config_path = \"yolov4.cfg\"\n",
    "labels_path = \"coco.names\"\n",
    "\n",
    "with open(labels_path, \"r\") as f:\n",
    "    labels = f.read().strip().split(\"\\n\")\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "layer_names = net.getLayerNames()\n",
    "out_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "# Detect persons using YOLO\n",
    "def detect_person_yolo(frame, conf_threshold=0.5, nms_threshold=0.4):\n",
    "    height, width = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(out_layers)\n",
    "\n",
    "    boxes, confidences = [], []\n",
    "    person_detected = False\n",
    "    \n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if labels[class_id] == \"person\" and confidence > conf_threshold:\n",
    "                box = detection[0:4] * np.array([width, height, width, height])\n",
    "                (centerX, centerY, w, h) = box.astype(\"int\")\n",
    "                x = int(centerX - w / 2)\n",
    "                y = int(centerY - h / 2)\n",
    "                boxes.append([x, y, int(w), int(h)])\n",
    "                confidences.append(float(confidence))\n",
    "                person_detected = True\n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Person {confidences[i]:.2f}', (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    print(f\"Person detected: {person_detected}\")\n",
    "    return person_detected, frame\n",
    "\n",
    "# Twilio SMS function\n",
    "def load_credentials():\n",
    "    try:\n",
    "        with open(\"credentialss.txt\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading credentials: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load credentials once at the start\n",
    "info_dict = load_credentials()\n",
    "if not info_dict:\n",
    "    print(\"Error: Could not load Twilio credentials! Exiting...\")\n",
    "    exit()  # Stop execution if credentials are missing\n",
    "\n",
    "print(\"Loaded Twilio credentials successfully:\", info_dict)  # Debugging\n",
    "\n",
    "def send_message(body):\n",
    "    try:\n",
    "        account_sid = info_dict['account_sid']\n",
    "        auth_token = info_dict['auth_token']\n",
    "        client = Client(account_sid, auth_token)\n",
    "        print(f\"Using Twilio SID: {account_sid}\")\n",
    "        print(f\"Using Twilio From Number: {info_dict['trial_num']}\")\n",
    "        print(f\"Sending SMS to: {info_dict['your_num']}\")\n",
    "        message = client.messages.create("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
